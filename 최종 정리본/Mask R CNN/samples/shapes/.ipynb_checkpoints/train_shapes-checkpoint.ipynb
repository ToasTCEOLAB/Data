{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAACWCAYAAABO+G6lAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAI+0lEQVR4nO3da6xlZ13H8d+/dKhNqSmk2BIvQL0UWhNo5KKApBAoiLEkUA3EahRMMEqDAWIEIVSKEkhfQFIwXEejJmoMNE2saawtbae2tClNkEsmNhReSOtQaCoww8iUPy/2mnAyOZ35d4a6z3g+n2Qyez1nnbWfPXle7O951j5T3R0AAICJE9Y9AQAA4PghIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABhbe0BU1ZOq6tpDxu46iutcXVXnLY9fVlX3V1Utx++tqt8aXOOyqvrKxvlU1XlVdXNV3VhV11XVWcv4WcvYp6rq+qr6icNc96er6o6q+lZVPW/D+Puq6tblz59sGH9LVd1eVbdV1Rsf7r8FAAA8UtYeED9Eu5I8d3n83CR3JDl3w/FNg2t8MMkLDhm7J8lLu/v5SS5P8mfL+B8k+Vh3n5/kr5Nccpjr3pPkxUn+6ZDxD3T3LyZ5TpKXL6FxapLXJDk4/vtVdcpg7mxDVfWodc8BANhejpuAqKoPVtVvV9UJVXVNVT37kFN2JTn40/2nJfnLJM+rqpOSnNHdXz7Sc3T3PUm+d8jYvd39zeVwf5IDy+PPJzltefzYJHuq6qSq2lVVT6mqM5cdhMd2997u/sYmz/efy9/fW677YJJ9Sb6a5OTlz74k3z3S3Nmaqurcqrpl2aX6l6o6Z1kX/1xV/1hVly7n3bXhez5aVecvj69Zdrluq6pfWsYuraq/qqqrkvxGVV1SVTctz/N7//evEgDYTk5c9wQWv1BVnzrCOW9Mcl1Wuwn/1t2fPuTrtyX5eFXtSNJZ7ThcnuRzSW5PkuUN2Ls3ufY7u/u6wz35sgvwriSvXYauTXJNVb02yUlJntXd+5fjnUkeSPJH3X3/EV5Xquo3k3zpYORU1dVJdmcVeO/q7v890jXYsl6SZGd3f7iqTkjyySRv6O5bquojg+9/RXd/u6qemuQDSV64jO/v7guX8cuTPD+r9XJTVX2yu7/+CLwWAIAtExB3dPeLDh5s9hmI7v5OVe1M8t4kT3iIr+9J8ookd3b3nqo6M6tdiV3LObckOf/hTm6Jkn9I8p7u/sIy/J4kb+vuT1TVq5P8RZI/7O7dVXV3ksd1978Prv2iJL+b5NeW459L8sokZ2X1hvCGqrqyu//r4c6bLWFnkj+tqr9L8tkkP5tV7CbJp5Ns9tmZg5/dOTnJ+6vq7Kx2p358wzkH19bPJzknyfXL8Y8m+ckkAoJjUlWvT3JRkru6284Wa2Edsm7W4Oa2SkAcUVU9Iauf/l+W1Zv1zT5cvCvJHyd563L81SS/ntUb9KPagVh+avy3Sa7s7is3finJfcvjPUket5z/4iQ7ktxXVRd291WHeU3PXl7Pr3T3vg3X/WZ371/O2Z/kMQ91Dba8/d395iRZPpz/30mekVU8PDOrz8ckyQNL8H4tydOT/E2SlyZ5sLt/uarOSbJxLT24/P3FJHcmeWV3d1Xt6G63vHHMuvuKJFesex5sb9Yh62YNbu64CIjlTfzOrG4JurWq/r6qXtbdVx9y6q4kb0py63J8c5KXZ3Ub0xF3IJbKfFWSpy5v9l6X5Lwkv5rkjKq6OMl/dPclWd3O9KGqOpBVMLyuqn4syZ9nddvKgSTXVtVnkvxPkk9k9ZPic6vq6u5+R5KPLU995fILo97U3Xcs97vfmlVMXN/du4/in42t4dVV9TtZ3VZ3b1br5qNV9fX8IECT1c7av2b12Zo9y9gtSd6yrMWbN7t4d39u+foNVfVgkn1LuB7Y7HwAgGNV3b3uOcC2tATpz3T3peueCwDA1HHzW5gAAID1swMBAACM2YEAAADGBAQAADB22N/C9JXLLnR/0zbyxLdfVeuew2ZOPu/11uE2su/OK7bcOrQGt5etuAYT63C7sQ7ZCh5qHdqBAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYO+z9Rc3j7v3Mg939t77qnMXb6mafkxB2PWvc0+GE7/afy5Gc+fd2zGLv7hhuSvQ+sexoAwFESEMfg/vv25uZr7l73NMYuuOjsnHqagPj/5knPeFo+884L1j2NscdffE8O7L593dMAAI6SW5gAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjJ247gkcz04/4zG54KKz1z2NsVNOffS6p8Aj4Ms33pjHX3zvuqcxduBLn133FACAYyAgjsGJO07Iqaf9yLqnwXa394Ec2H37umcBAGwTbmECAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMCQgAAGBMQAAAAGMCAgAAGBMQAADAmIAAAADGBAQAADAmIAAAgDEBAQAAjAkIAABgTEAAAABjAgIAABgTEAAAwJiAAAAAxgQEAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAYExAAAMCYgAAAAMYEBAAAMCYgAACAMQEBAACMVXevew4AAMBxwg4EAAAwJiAAAIAxAQEAAIwJCAAAYExAAAAAYwICAAAY+z4FZ9ERXAo8GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1208: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\user\\Desktop\\LeeSac\\Mask_RCNN\\logs\\shapes20220129T2019\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             layers='heads')\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\LeeSac\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m         )\n\u001b[0;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2065\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[0mall_finished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mall_finished\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
