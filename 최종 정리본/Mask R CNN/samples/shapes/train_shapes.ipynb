{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1208: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: C:\\Users\\user\\Desktop\\LeeSac\\Mask_RCNN\\logs\\shapes20220129T2025\\mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-83fb3ae74319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             layers='heads')\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\LeeSac\\Mask_RCNN\\mrcnn\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers, augmentation, custom_callbacks, no_augmentation_sources)\u001b[0m\n\u001b[0;32m   2372\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2374\u001b[1;33m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2375\u001b[0m         )\n\u001b[0;32m   2376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2063\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2065\u001b[1;33m                     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Mask-R-CNN-DEMO-tf-150\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[0mall_finished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mall_finished\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
